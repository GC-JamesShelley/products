---
# Source: zeebe-full/charts/zeebe/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: zeebe-full/charts/operate/templates/configmap.yaml
kind: ConfigMap
metadata:
  name: zeebe-operate
apiVersion: v1
data:
  application.yml: |
    # Operate configuration file
    camunda.operate:
      # ELS instance to store Operate data
      elasticsearch:
        # Cluster name
        clusterName: elasticsearch
        # Host
        host: elasticsearch-master
        # Transport port
        port: 9200
      # Zeebe instance
      zeebe:
        # Broker contact point
        brokerContactPoint: zeebe-zeebe:26500
      # ELS instance to export Zeebe data to
      zeebeElasticsearch:
        # Cluster name
        clusterName: elasticsearch
        # Host
        host: elasticsearch-master
        # Transport port
        port: 9200
        # Index prefix, configured in Zeebe Elasticsearch exporter
        prefix: zeebe-record
    logging:
      level:
        ROOT: INFO
        org.camunda.operate: DEBUG
    #Spring Boot Actuator endpoints to be exposed
    management.endpoints.web.exposure.include: health,info,conditions,configprops,prometheus
---
# Source: zeebe-full/charts/zeebe/templates/configmap.yaml
kind: ConfigMap
metadata:
  name: "zeebe-zeebe"
  labels: 
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: zeebe
    app: "zeebe-zeebe"
apiVersion: v1
data:
  startup.sh: |
    #!/bin/bash -xeu

    configFile=/usr/local/zeebe/conf/zeebe.cfg.toml
    export ZEEBE_HOST=$(hostname -f)
    export ZEEBE_NODE_ID="${HOSTNAME##*-}"
    
    # We need to specify all brokers as contact points for partition healing to work correctly
    # https://github.com/zeebe-io/zeebe/issues/2684
    ZEEBE_CONTACT_POINTS=${HOSTNAME::-1}0.$(hostname -d):26502
    for (( i=1; i<$ZEEBE_CLUSTER_SIZE; i++ ))
    do
        ZEEBE_CONTACT_POINTS="${ZEEBE_CONTACT_POINTS},${HOSTNAME::-1}$i.$(hostname -d):26502"
    done
    export ZEEBE_CONTACT_POINTS="${ZEEBE_CONTACT_POINTS}"
    
    exec /usr/local/zeebe/bin/broker
  zeebe.cfg.toml: |
    
    # For more information about this configuration visit: https://docs.zeebe.io/operations/the-zeebecfgtoml-file.html
    [threads]
    cpuThreadCount = "2"
    ioThreadCount = "2"
    [gateway.monitoring]
    enabled = false
    [[exporters]]
    id = "elasticsearch"
    className = "io.zeebe.exporter.ElasticsearchExporter"
      [exporters.args]
      url = "http://elasticsearch-master:9200"

      [exporters.args.bulk]
      delay = 5
      size = 1_000

      #[exporters.args.authentication]
      #username = elastic
      #password = changeme

      [exporters.args.index]
      prefix = "zeebe-record"
      createTemplate = true

      command = false
      event = true
      rejection = false

      deployment = true
      incident = true
      job = true
      message = false
      messageSubscription = false
      raft = false
      workflowInstance = true
      workflowInstanceSubscription = false
---
# Source: zeebe-full/charts/nginx-ingress/templates/controller-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress
---
# Source: zeebe-full/charts/nginx-ingress/templates/default-backend-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress-backend
---
# Source: zeebe-full/charts/nginx-ingress/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses/status
    verbs:
      - update
---
# Source: zeebe-full/charts/nginx-ingress/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: zeebe-nginx-ingress
subjects:
  - kind: ServiceAccount
    name: zeebe-nginx-ingress
    namespace: default
---
# Source: zeebe-full/charts/nginx-ingress/templates/controller-role.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - ingress-controller-leader-nginx
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: zeebe-full/charts/nginx-ingress/templates/controller-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: zeebe-nginx-ingress
subjects:
  - kind: ServiceAccount
    name: zeebe-nginx-ingress
    namespace: default
---
# Source: zeebe-full/charts/nginx-ingress/templates/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    component: "controller"
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress-controller
spec:
  
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
  selector:
    app: nginx-ingress
    component: "controller"
    release: zeebe
  type: "LoadBalancer"
---
# Source: zeebe-full/charts/nginx-ingress/templates/default-backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    component: "default-backend"
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress-default-backend
spec:
  
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app: nginx-ingress
    component: "default-backend"
    release: zeebe
  type: "ClusterIP"
---
# Source: zeebe-full/charts/operate/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: zeebe-operate
  labels:
    app: zeebe-operate
spec:
  type: ClusterIP
  ports:
  - port: 80
    name: http
    targetPort: 8080
    protocol: TCP
  selector:
    app: zeebe-operate
---
# Source: zeebe-full/charts/zeebe/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "zeebe"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    heritage: "Helm"
    release: "zeebe"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: zeebe-full/charts/zeebe/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "zeebe"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: zeebe-full/charts/zeebe/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "zeebe-zeebe"
  labels:
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: zeebe
    app: "zeebe-zeebe"
    app: zeebe
  annotations:
    null    
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
    - port: 9600
      protocol: TCP
      name: http  
    - port: 26500
      protocol: TCP
      name: gateway
  selector:
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: zeebe
    app: "zeebe-zeebe"
---
# Source: zeebe-full/charts/nginx-ingress/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    component: "controller"
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress-controller
spec:
  selector:
    matchLabels:
      app: nginx-ingress
      release: zeebe
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    {}
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app: nginx-ingress
        component: "controller"
        release: zeebe
    spec:
      dnsPolicy: ClusterFirst
      containers:
        - name: nginx-ingress-controller
          image: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - /nginx-ingress-controller
            - --default-backend-service=default/zeebe-nginx-ingress-default-backend
            - --election-id=ingress-controller-leader
            - --ingress-class=nginx
            - --configmap=default/zeebe-nginx-ingress-controller
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            runAsUser: 33
            allowPrivilegeEscalation: true
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          resources:
            {}
      hostNetwork: false
      serviceAccountName: zeebe-nginx-ingress
      terminationGracePeriodSeconds: 60
---
# Source: zeebe-full/charts/nginx-ingress/templates/default-backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx-ingress
    chart: nginx-ingress-1.26.2
    component: "default-backend"
    heritage: Helm
    release: zeebe
  name: zeebe-nginx-ingress-default-backend
spec:
  selector:
    matchLabels:
      app: nginx-ingress
      release: zeebe
  replicas: 1
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:
        app: nginx-ingress
        component: "default-backend"
        release: zeebe
    spec:
      containers:
        - name: nginx-ingress-default-backend
          image: "k8s.gcr.io/defaultbackend-amd64:1.5"
          imagePullPolicy: "IfNotPresent"
          args:
          securityContext:
            runAsUser: 65534
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
      serviceAccountName: zeebe-nginx-ingress-backend
      terminationGracePeriodSeconds: 60
---
# Source: zeebe-full/charts/operate/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zeebe-operate
  labels:
    app: zeebe-operate
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zeebe-operate
  template:
    metadata:
      labels:
        app: zeebe-operate
    spec:
      containers:
      - name: operate
        image: "camunda/operate:1.2.0"
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 768Mi
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        volumeMounts:
        - name: config
          mountPath: /usr/local/operate/config/application.yml
          subPath: application.yml
      volumes:
      - name: config
        configMap:
          name: zeebe-operate
          defaultMode: 0744
      securityContext:
        null
---
# Source: zeebe-full/charts/zeebe/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "zeebe"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "6"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1G
      storageClassName: standard
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        heritage: "Helm"
        release: "zeebe"
        chart: "elasticsearch"
        app: "elasticsearch-master"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      - name: create
        image: busybox:1.28
        command: ["mkdir", "-p", "/usr/share/elasticsearch/data/nodes/"]
        securityContext:
          runAsUser: 0
        volumeMounts:
          - mountPath: /usr/share/elasticsearch/data
            name: elasticsearch-master
      - name: file-permissions
        image: busybox:1.28
        command: ["chown", "-R", "1000:1000", "/usr/share/elasticsearch/"]
        securityContext:
          runAsUser: 0
        volumeMounts:
          - mountPath: /usr/share/elasticsearch/data
            name: elasticsearch-master
      
      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: 'wait_for_status=green&timeout=1s' )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                http () {
                    local path="${1}"
                    if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                      BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                    else
                      BASIC_AUTH=''
                    fi
                    curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path}
                }

                if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy and there are master nodes available'
                    http "/_cluster/health?timeout=0s"
                else
                    echo 'Waiting for elasticsearch cluster to become cluster to be ready (request params: "wait_for_status=green&timeout=1s" )'
                    if http "/_cluster/health?wait_for_status=green&timeout=1s" ; then
                        touch ${START_FILE}
                        exit 0
                    else
                        echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                        exit 1
                    fi
                fi
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 512M
          requests:
            cpu: 100m
            memory: 512M
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.zen.minimum_master_nodes
            value: "2"
          - name: discovery.zen.ping.unicast.hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx128m -Xms128m"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
        volumeMounts:
          - name: "elasticsearch-master"
            mountPath: /usr/share/elasticsearch/data
---
# Source: zeebe-full/charts/zeebe/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "zeebe-zeebe"
  labels:
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: zeebe
    app: "zeebe-zeebe"
    app: zeebe
  annotations:   
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: zeebe
      app.kubernetes.io/instance: zeebe
      app: "zeebe-zeebe"
  serviceName: "zeebe-zeebe"
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app.kubernetes.io/name: zeebe
        app.kubernetes.io/instance: zeebe
        app: "zeebe-zeebe"
      annotations:   
    spec:
      initContainers:    
      containers:
      - name: zeebe
        image: "camunda/zeebe:0.22.1"
        imagePullPolicy: IfNotPresent
        env:
        - name: ZEEBE_LOG_LEVEL
          value: debug
        - name: ZEEBE_PARTITIONS_COUNT
          value: "3"
        - name: ZEEBE_CLUSTER_SIZE
          value: "3"
        - name: ZEEBE_REPLICATION_FACTOR
          value: "3"
        - name: JAVA_TOOL_OPTIONS
          value:
            "-XX:+UseParallelGC \n-XX:MinHeapFreeRatio=5\n-XX:MaxHeapFreeRatio=10\n-XX:MaxRAMPercentage=25.0
              \n-XX:GCTimeRatio=4 \n-XX:AdaptiveSizePolicyWeight=90\n-XX:+PrintFlagsFinal\n-Xmx4g\n-Xms4g\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/usr/local/zeebe/data\n-XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log\n"
        ports:
        - containerPort: 9600
          name: http
        - containerPort: 26500
          name: gateway
        - containerPort: 26501
          name: command
        - containerPort: 26502
          name: internal
        readinessProbe:
          httpGet:
            path: /ready
            port: 9600
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
            limits:
              cpu: 1000m
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 2Gi
        volumeMounts:
        - name: config
          mountPath: /usr/local/zeebe/conf/zeebe.cfg.toml
          subPath: zeebe.cfg.toml
        - name: config
          mountPath: /usr/local/bin/startup.sh
          subPath: startup.sh
        - name: data
          mountPath: /usr/local/zeebe/data
      volumes:
      - name: config
        configMap:
          name: "zeebe-zeebe"
          defaultMode: 0744
      securityContext:
        null
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ReadWriteOnce]
      resources:
        requests:
          storage: "10Gi"
---
# Source: zeebe-full/charts/operate/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: zeebe-operate
  labels: 
    app.kubernetes.io/name: operate
    helm.sh/chart: operate-0.0.17
    app.kubernetes.io/instance: zeebe
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
  annotations: 
    ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: 
      http:
        paths:
          - path: /
            backend:
              serviceName: zeebe-operate
              servicePort: 80
---
# Source: zeebe-full/charts/operate/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "zeebe-operate-test-connection"
  labels:
    app.kubernetes.io/name: operate
    helm.sh/chart: operate-0.0.17
    app.kubernetes.io/instance: zeebe
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args:  ['zeebe-operate:80']
  restartPolicy: Never
---
# Source: zeebe-full/charts/zeebe/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "zeebe-ykihw-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: "zeebe-xljsi-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:6.8.3"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: zeebe-full/charts/zeebe/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "{{ .Release.Name }}-zeebe-test-connection"
  labels:
    app.kubernetes.io/name: zeebe
    helm.sh/chart: zeebe-0.0.68
    app.kubernetes.io/instance: zeebe
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args:  ['{{ .Release.Name }}-zeebe:9600']
  restartPolicy: Never
